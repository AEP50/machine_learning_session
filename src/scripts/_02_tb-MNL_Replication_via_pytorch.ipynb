{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "This notebook has three goals.\n",
    "\n",
    "First: to recreate the authors' MNL model using PyTorch.\n",
    "This will give me the ability to easily extend their model as I please.\n",
    "\n",
    "Second: to augment the author's MNL model with a neural network that implements a piecewise linear spline for `trans_access`, the sole continuous variable in the model. My intuition is that a piecewise linear specification (with structural constraints on the parameter signs to avoid illogical elasticities) will be an improvement over the MNL.\n",
    "\n",
    "Third: To augment the model described above with a Random Forest to capture the effect of discrete variables, beyond that of the main-effects that the author's have already included in their specification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pylogit as pl\n",
    "\n",
    "import seaborn as sbn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Use the following for optimization\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# PyTorch is used for general numeric computation\n",
    "# with automatic differentiation\n",
    "import torch\n",
    "# Used to access various pytorch utilities\n",
    "import torch.nn as nn\n",
    "# Used to get sparse matrices =)\n",
    "import torch.sparse\n",
    "\n",
    "# Used to build the random forest addition to the choice model\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recreate the original MNL model using PyLogit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "train_df_all =\\\n",
    "    pd.read_csv('./data/mnl_training_data_long_all_sample.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create needed columns\n",
    "for df in [train_df_all]:\n",
    "    df['num_adults_eq_2'] = (df['num_adults'] == 2).astype(int)\n",
    "    df['num_adults_gte_3'] = (df['num_adults'] >= 3).astype(int)\n",
    "    df['sectr_1'] = (df['sectr'] == 1).astype(int)\n",
    "    df['sectr_2'] = (df['sectr'] == 2).astype(int)\n",
    "    df['income_1'] = (df['incom'] == 1).astype(int)\n",
    "    df['income_2'] = (df['incom'] == 2).astype(int)\n",
    "    df['income_4'] = (df['incom'] == 4).astype(int)\n",
    "    df['income_5'] = (df['incom'] == 5).astype(int)\n",
    "    df['income_gt_5'] = (df['incom'] > 5).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create labels for the various alternatives\n",
    "choice_labels =\\\n",
    "    {1: '0 Cars',\n",
    "     2: '1 Car',\n",
    "     3: '2 Cars',\n",
    "     4: '3+ Cars',\n",
    "    }\n",
    "\n",
    "# Create the specification dictionaries\n",
    "spec_dict, name_dict = OrderedDict(), OrderedDict()\n",
    "\n",
    "spec_dict['intercept'] = [2, 3, 4]\n",
    "spec_dict['num_adults_eq_2'] = [1, 2, 4]\n",
    "spec_dict['num_adults_gte_3'] = [1, 2, 3]\n",
    "spec_dict['sectr_1'] = [2, 3, 4]\n",
    "spec_dict['sectr_2'] = [2, 3, 4]\n",
    "spec_dict['income_1'] = [2, 3, 4]\n",
    "spec_dict['income_2'] = [2, 3, 4]\n",
    "spec_dict['income_4'] = [2, 3, 4]\n",
    "spec_dict['income_5'] = [2, 3, 4]\n",
    "spec_dict['income_gt_5'] = [2, 3, 4]\n",
    "spec_dict['tran_access'] = [2, 3, 4]\n",
    "spec_dict['numWorkers'] = [2, 3, 4]\n",
    "\n",
    "for col in spec_dict:\n",
    "    name_dict[col] =\\\n",
    "        [col + ' ({})'.format(choice_labels[x]) for x in spec_dict[col]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tbrathwaite/anaconda2/lib/python2.7/site-packages/pylogit/choice_tools.py:703: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  design_matrix = np.hstack((x[:, None] for x in independent_vars))\n"
     ]
    }
   ],
   "source": [
    "# Take note of important data columns\n",
    "ALT_ID_COL = 'altid'\n",
    "CHOICE_COL = 'choiceBoolean'\n",
    "OBS_ID_COL = 'sampn'\n",
    "\n",
    "# Create the model object\n",
    "mnl_model =\\\n",
    "    pl.create_choice_model(train_df_all,\n",
    "                           ALT_ID_COL,\n",
    "                           OBS_ID_COL,\n",
    "                           CHOICE_COL,\n",
    "                           spec_dict,\n",
    "                           'MNL',\n",
    "                           names=name_dict,\n",
    "                          )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-likelihood at zero: -14,528.3649\n",
      "Initial Log-likelihood: -14,528.3649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tbrathwaite/anaconda2/lib/python2.7/site-packages/scipy/optimize/_minimize.py:506: RuntimeWarning: Method BFGS does not use Hessian information (hess).\n",
      "  RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimation Time for Point Estimation: 0.56 seconds.\n",
      "Final log-likelihood: -8,419.3714\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Multinomial Logit Model Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>      <td>choiceBoolean</td>      <th>  No. Observations:  </th>   <td>10,480</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>         <td>Multinomial Logit Model</td> <th>  Df Residuals:      </th>   <td>10,444</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                  <td>MLE</td>           <th>  Df Model:          </th>     <td>36</td>     \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 13 Jan 2020</td>     <th>  Pseudo R-squ.:     </th>    <td>0.420</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>06:47:36</td>         <th>  Pseudo R-bar-squ.: </th>    <td>0.418</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>AIC:</th>                 <td>16,910.743</td>        <th>  Log-Likelihood:    </th> <td>-8,419.371</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>BIC:</th>                 <td>17,172.003</td>        <th>  LL-Null:           </th> <td>-14,528.365</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "              <td></td>                 <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept (1 Car)</th>         <td>    2.6264</td> <td>    0.255</td> <td>   10.311</td> <td> 0.000</td> <td>    2.127</td> <td>    3.126</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept (2 Cars)</th>        <td>    1.0124</td> <td>    0.469</td> <td>    2.159</td> <td> 0.031</td> <td>    0.093</td> <td>    1.931</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept (3+ Cars)</th>       <td>   -1.9606</td> <td>    1.221</td> <td>   -1.606</td> <td> 0.108</td> <td>   -4.353</td> <td>    0.432</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>num_adults_eq_2 (0 Cars)</th>  <td>   -3.4084</td> <td>    0.331</td> <td>  -10.288</td> <td> 0.000</td> <td>   -4.058</td> <td>   -2.759</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>num_adults_eq_2 (1 Car)</th>   <td>   -3.0713</td> <td>    0.327</td> <td>   -9.400</td> <td> 0.000</td> <td>   -3.712</td> <td>   -2.431</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>num_adults_eq_2 (3+ Cars)</th> <td>   -1.8843</td> <td>    1.065</td> <td>   -1.769</td> <td> 0.077</td> <td>   -3.973</td> <td>    0.204</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>num_adults_gte_3 (0 Cars)</th> <td>   -3.8212</td> <td>    1.022</td> <td>   -3.740</td> <td> 0.000</td> <td>   -5.824</td> <td>   -1.819</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>num_adults_gte_3 (1 Car)</th>  <td>   -3.6161</td> <td>    1.019</td> <td>   -3.549</td> <td> 0.000</td> <td>   -5.613</td> <td>   -1.619</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>num_adults_gte_3 (2 Cars)</th> <td>   -0.5120</td> <td>    1.063</td> <td>   -0.482</td> <td> 0.630</td> <td>   -2.596</td> <td>    1.572</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sectr_1 (1 Car)</th>           <td>    0.5661</td> <td>    0.092</td> <td>    6.138</td> <td> 0.000</td> <td>    0.385</td> <td>    0.747</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sectr_1 (2 Cars)</th>          <td>    0.2848</td> <td>    0.156</td> <td>    1.831</td> <td> 0.067</td> <td>   -0.020</td> <td>    0.590</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sectr_1 (3+ Cars)</th>         <td>    0.6836</td> <td>    0.305</td> <td>    2.239</td> <td> 0.025</td> <td>    0.085</td> <td>    1.282</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sectr_2 (1 Car)</th>           <td>   -1.5095</td> <td>    0.064</td> <td>  -23.608</td> <td> 0.000</td> <td>   -1.635</td> <td>   -1.384</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sectr_2 (2 Cars)</th>          <td>   -2.9781</td> <td>    0.128</td> <td>  -23.285</td> <td> 0.000</td> <td>   -3.229</td> <td>   -2.727</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sectr_2 (3+ Cars)</th>         <td>   -3.2721</td> <td>    0.387</td> <td>   -8.456</td> <td> 0.000</td> <td>   -4.031</td> <td>   -2.514</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>income_1 (1 Car)</th>          <td>   -1.8959</td> <td>    0.097</td> <td>  -19.483</td> <td> 0.000</td> <td>   -2.087</td> <td>   -1.705</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>income_1 (2 Cars)</th>         <td>   -3.5720</td> <td>    0.359</td> <td>   -9.959</td> <td> 0.000</td> <td>   -4.275</td> <td>   -2.869</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>income_1 (3+ Cars)</th>        <td>   -3.5108</td> <td>    1.047</td> <td>   -3.352</td> <td> 0.001</td> <td>   -5.563</td> <td>   -1.458</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>income_2 (1 Car)</th>          <td>   -0.9951</td> <td>    0.071</td> <td>  -14.090</td> <td> 0.000</td> <td>   -1.134</td> <td>   -0.857</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>income_2 (2 Cars)</th>         <td>   -1.7182</td> <td>    0.135</td> <td>  -12.737</td> <td> 0.000</td> <td>   -1.983</td> <td>   -1.454</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>income_2 (3+ Cars)</th>        <td>   -2.2863</td> <td>    0.439</td> <td>   -5.205</td> <td> 0.000</td> <td>   -3.147</td> <td>   -1.425</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>income_4 (1 Car)</th>          <td>    0.7917</td> <td>    0.106</td> <td>    7.469</td> <td> 0.000</td> <td>    0.584</td> <td>    0.999</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>income_4 (2 Cars)</th>         <td>    1.5112</td> <td>    0.123</td> <td>   12.328</td> <td> 0.000</td> <td>    1.271</td> <td>    1.751</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>income_4 (3+ Cars)</th>        <td>    1.8265</td> <td>    0.246</td> <td>    7.423</td> <td> 0.000</td> <td>    1.344</td> <td>    2.309</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>income_5 (1 Car)</th>          <td>    1.3261</td> <td>    0.209</td> <td>    6.343</td> <td> 0.000</td> <td>    0.916</td> <td>    1.736</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>income_5 (2 Cars)</th>         <td>    3.1611</td> <td>    0.215</td> <td>   14.737</td> <td> 0.000</td> <td>    2.741</td> <td>    3.582</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>income_5 (3+ Cars)</th>        <td>    3.8959</td> <td>    0.297</td> <td>   13.112</td> <td> 0.000</td> <td>    3.314</td> <td>    4.478</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>income_gt_5 (1 Car)</th>       <td>   -0.2626</td> <td>    0.089</td> <td>   -2.961</td> <td> 0.003</td> <td>   -0.436</td> <td>   -0.089</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>income_gt_5 (2 Cars)</th>      <td>    0.2713</td> <td>    0.118</td> <td>    2.295</td> <td> 0.022</td> <td>    0.040</td> <td>    0.503</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>income_gt_5 (3+ Cars)</th>     <td>    0.7713</td> <td>    0.274</td> <td>    2.817</td> <td> 0.005</td> <td>    0.235</td> <td>    1.308</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>tran_access (1 Car)</th>       <td>   -0.3569</td> <td>    0.038</td> <td>   -9.512</td> <td> 0.000</td> <td>   -0.430</td> <td>   -0.283</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>tran_access (2 Cars)</th>      <td>   -0.8650</td> <td>    0.054</td> <td>  -15.964</td> <td> 0.000</td> <td>   -0.971</td> <td>   -0.759</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>tran_access (3+ Cars)</th>     <td>   -0.8694</td> <td>    0.114</td> <td>   -7.635</td> <td> 0.000</td> <td>   -1.093</td> <td>   -0.646</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>numWorkers (1 Car)</th>        <td>    0.3593</td> <td>    0.036</td> <td>    9.897</td> <td> 0.000</td> <td>    0.288</td> <td>    0.430</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>numWorkers (2 Cars)</th>       <td>    0.7613</td> <td>    0.050</td> <td>   15.102</td> <td> 0.000</td> <td>    0.663</td> <td>    0.860</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>numWorkers (3+ Cars)</th>      <td>    1.1392</td> <td>    0.084</td> <td>   13.540</td> <td> 0.000</td> <td>    0.974</td> <td>    1.304</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                     Multinomial Logit Model Regression Results                    \n",
       "===================================================================================\n",
       "Dep. Variable:               choiceBoolean   No. Observations:               10,480\n",
       "Model:             Multinomial Logit Model   Df Residuals:                   10,444\n",
       "Method:                                MLE   Df Model:                           36\n",
       "Date:                     Mon, 13 Jan 2020   Pseudo R-squ.:                   0.420\n",
       "Time:                             06:47:36   Pseudo R-bar-squ.:               0.418\n",
       "AIC:                            16,910.743   Log-Likelihood:             -8,419.371\n",
       "BIC:                            17,172.003   LL-Null:                   -14,528.365\n",
       "=============================================================================================\n",
       "                                coef    std err          z      P>|z|      [0.025      0.975]\n",
       "---------------------------------------------------------------------------------------------\n",
       "intercept (1 Car)             2.6264      0.255     10.311      0.000       2.127       3.126\n",
       "intercept (2 Cars)            1.0124      0.469      2.159      0.031       0.093       1.931\n",
       "intercept (3+ Cars)          -1.9606      1.221     -1.606      0.108      -4.353       0.432\n",
       "num_adults_eq_2 (0 Cars)     -3.4084      0.331    -10.288      0.000      -4.058      -2.759\n",
       "num_adults_eq_2 (1 Car)      -3.0713      0.327     -9.400      0.000      -3.712      -2.431\n",
       "num_adults_eq_2 (3+ Cars)    -1.8843      1.065     -1.769      0.077      -3.973       0.204\n",
       "num_adults_gte_3 (0 Cars)    -3.8212      1.022     -3.740      0.000      -5.824      -1.819\n",
       "num_adults_gte_3 (1 Car)     -3.6161      1.019     -3.549      0.000      -5.613      -1.619\n",
       "num_adults_gte_3 (2 Cars)    -0.5120      1.063     -0.482      0.630      -2.596       1.572\n",
       "sectr_1 (1 Car)               0.5661      0.092      6.138      0.000       0.385       0.747\n",
       "sectr_1 (2 Cars)              0.2848      0.156      1.831      0.067      -0.020       0.590\n",
       "sectr_1 (3+ Cars)             0.6836      0.305      2.239      0.025       0.085       1.282\n",
       "sectr_2 (1 Car)              -1.5095      0.064    -23.608      0.000      -1.635      -1.384\n",
       "sectr_2 (2 Cars)             -2.9781      0.128    -23.285      0.000      -3.229      -2.727\n",
       "sectr_2 (3+ Cars)            -3.2721      0.387     -8.456      0.000      -4.031      -2.514\n",
       "income_1 (1 Car)             -1.8959      0.097    -19.483      0.000      -2.087      -1.705\n",
       "income_1 (2 Cars)            -3.5720      0.359     -9.959      0.000      -4.275      -2.869\n",
       "income_1 (3+ Cars)           -3.5108      1.047     -3.352      0.001      -5.563      -1.458\n",
       "income_2 (1 Car)             -0.9951      0.071    -14.090      0.000      -1.134      -0.857\n",
       "income_2 (2 Cars)            -1.7182      0.135    -12.737      0.000      -1.983      -1.454\n",
       "income_2 (3+ Cars)           -2.2863      0.439     -5.205      0.000      -3.147      -1.425\n",
       "income_4 (1 Car)              0.7917      0.106      7.469      0.000       0.584       0.999\n",
       "income_4 (2 Cars)             1.5112      0.123     12.328      0.000       1.271       1.751\n",
       "income_4 (3+ Cars)            1.8265      0.246      7.423      0.000       1.344       2.309\n",
       "income_5 (1 Car)              1.3261      0.209      6.343      0.000       0.916       1.736\n",
       "income_5 (2 Cars)             3.1611      0.215     14.737      0.000       2.741       3.582\n",
       "income_5 (3+ Cars)            3.8959      0.297     13.112      0.000       3.314       4.478\n",
       "income_gt_5 (1 Car)          -0.2626      0.089     -2.961      0.003      -0.436      -0.089\n",
       "income_gt_5 (2 Cars)          0.2713      0.118      2.295      0.022       0.040       0.503\n",
       "income_gt_5 (3+ Cars)         0.7713      0.274      2.817      0.005       0.235       1.308\n",
       "tran_access (1 Car)          -0.3569      0.038     -9.512      0.000      -0.430      -0.283\n",
       "tran_access (2 Cars)         -0.8650      0.054    -15.964      0.000      -0.971      -0.759\n",
       "tran_access (3+ Cars)        -0.8694      0.114     -7.635      0.000      -1.093      -0.646\n",
       "numWorkers (1 Car)            0.3593      0.036      9.897      0.000       0.288       0.430\n",
       "numWorkers (2 Cars)           0.7613      0.050     15.102      0.000       0.663       0.860\n",
       "numWorkers (3+ Cars)          1.1392      0.084     13.540      0.000       0.974       1.304\n",
       "=============================================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "num_params_mnl = mnl_model.design.shape[1]\n",
    "init_values = np.zeros(num_params_mnl)\n",
    "\n",
    "mnl_model.fit_mle(init_values)\n",
    "\n",
    "# Show model estimation results\n",
    "mnl_model.get_statsmodels_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recreate the original MNL model using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert training and testing data to pytorch format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast all arrays as 2D arrays of float32 elements\n",
    "# because that's what pytorch likes\n",
    "x_train = mnl_model.design.astype(np.float32)\n",
    "y_train = mnl_model.choices.astype(np.float32)[:, None]\n",
    "\n",
    "# 2.3 Convert the data from numpy arrays into pytorch variables\n",
    "x_train_torch = torch.from_numpy(x_train)\n",
    "y_train_torch = torch.from_numpy(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create needed helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.0 Make helper functions needed for model training\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def get_numpy_grad(model):\n",
    "    return np.concatenate(list(x.grad.data.numpy().ravel()\n",
    "                               for x in model.parameters()),\n",
    "                          axis=0)\n",
    "\n",
    "def get_numpy_params(model):\n",
    "    return np.concatenate(list(x.data.numpy().ravel()\n",
    "                               for x in model.parameters()),\n",
    "                          axis=0)\n",
    "\n",
    "\n",
    "def vec_to_state_dict(vec,\n",
    "                      model,  \n",
    "                      param_shapes=None, # keys = param names; vals = size tuple\n",
    "                     ):\n",
    "    # Figure out the number of parameters in the model\n",
    "    total_num_params = count_parameters(model)\n",
    "    if vec.shape[0] != total_num_params:\n",
    "        msg = '`vec` and `model` MUST have the same number of parameters.'\n",
    "        raise ValueError(msg)\n",
    "    # Create a new state dict\n",
    "    new_state = deepcopy(model.state_dict())\n",
    "    # Pointer for slicing the vector for each parameter\n",
    "    pointer = 0\n",
    "    for pos, param in enumerate(model.parameters()):\n",
    "        # The length of the parameter\n",
    "        num_param = param.numel()\n",
    "        # Get the name of the current parameter\n",
    "        param_name = list(new_state.keys())[pos]\n",
    "        # Slice the vector, reshape it, and replace the old values\n",
    "        if num_param == 1:\n",
    "            new_state[param_name] =\\\n",
    "                vec[pointer:pointer + num_param].double()\n",
    "        else:\n",
    "            if param_shapes is not None:\n",
    "                shape_tuple = param_shapes[param_name]\n",
    "            else:\n",
    "                shape_tuple = (1, -1)\n",
    "            new_state[param_name] =\\\n",
    "                vec[pointer:pointer + num_param].view(*shape_tuple).double()\n",
    "        # Increment the pointer\n",
    "        pointer += num_param\n",
    "    return new_state\n",
    "\n",
    "\n",
    "def set_model_params(params,\n",
    "                     model,\n",
    "                     param_shapes=None, # keys = param names vals = size tuple\n",
    "                    ):\n",
    "    # Convert the numpy array torch a pytorch tensor\n",
    "    if isinstance(params, np.ndarray):\n",
    "        param_tensor =\\\n",
    "            (torch.tensor(params,\n",
    "                          requires_grad=True)\n",
    "                  .double())\n",
    "    else:\n",
    "        param_tensor = params\n",
    "    # Conver the vector into a pytorch state dict\n",
    "    current_state_dict =\\\n",
    "        vec_to_state_dict(param_tensor, model, param_shapes=param_shapes)\n",
    "    # Load the new state dict onto the model\n",
    "    model.load_state_dict(current_state_dict)\n",
    "    return None\n",
    "        \n",
    "\n",
    "def make_scipy_closure(\n",
    "        design,\n",
    "        targets,\n",
    "        model,\n",
    "        loss_func,\n",
    "        param_shapes=None, # keys = param names vals = size tuple\n",
    "        ):\n",
    "    def closure(params):\n",
    "        # params -> loss, grad\n",
    "        # Load the parameters onto the model\n",
    "        set_model_params(params, model, param_shapes=param_shapes)\n",
    "        # Ensure the gradients are summed starting from zero\n",
    "        model.zero_grad()\n",
    "        # Calculate the loss\n",
    "        loss =\\\n",
    "            loss_func(model(design), targets)\n",
    "        # Compute the gradients\n",
    "        loss.backward()\n",
    "        # Get the gradient.\n",
    "        grad = get_numpy_grad(model)\n",
    "        # Get a value of the loss to pass around?\n",
    "        loss_val = loss.detach().numpy()\n",
    "        return loss_val, grad\n",
    "    return closure\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make the PyTorch MNL class using mapping matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Set up the model in pytorch again\n",
    "num_beta_params = x_train_torch.shape[1]\n",
    "num_alts = 3\n",
    "\n",
    "class MNL_Torch_v2(nn.Module):\n",
    "    \"\"\"\n",
    "    Uses sparse matrices\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 num_index_params,\n",
    "                 num_alternatives,\n",
    "                 rows_to_obs, # torch.sparse.FloatTensor\n",
    "                 min_exponent_val=-700,\n",
    "                 max_exponent_val=700,\n",
    "                 max_comp_value=1e300,\n",
    "                 max_sub_1=1-1e16,\n",
    "                 min_comp_value=1e-300):\n",
    "        super(MNL_Torch_v2, self).__init__()\n",
    "        # Store needed constants for the model\n",
    "        self.num_alternatives = int(num_alternatives)\n",
    "        self.num_index_params = int(num_index_params)\n",
    "        self.rows_to_obs = rows_to_obs\n",
    "        \n",
    "        # Store constants related to avoiding numerical overflow or underflow\n",
    "        self.min_exponent_val = min_exponent_val\n",
    "        self.max_exponent_val = max_exponent_val\n",
    "        self.min_comp_value = min_comp_value\n",
    "        self.max_sub_1 = max_sub_1\n",
    "        self.max_comp_value = max_comp_value\n",
    "\n",
    "        # Store computation layers as attributes\n",
    "        self.linear =\\\n",
    "            nn.Linear(num_index_params, 1, bias=False)\n",
    "        return None\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Note we use clamp to guard against later over- or under-flow.\n",
    "        sys_utilities =\\\n",
    "            torch.clamp(self.calc_sys_utilities(inputs),\n",
    "                        min=self.min_exponent_val,\n",
    "                        max=self.max_exponent_val)\n",
    "\n",
    "        exponentiated_sys_utilities = torch.exp(sys_utilities)\n",
    "\n",
    "        # Get the denominators for calculating the probabilities\n",
    "        # There will be one entry per observation\n",
    "        denominators_by_obs =\\\n",
    "            torch.sparse.mm(self.rows_to_obs.transpose(0, 1),\n",
    "                            exponentiated_sys_utilities)\n",
    "        # Convert the denominators into an array of length sys_utilities.size[0]\n",
    "        long_denominators = torch.sparse.mm(self.rows_to_obs, denominators_by_obs)\n",
    "\n",
    "        # Note we use clamp to guard against against zero probabilities.\n",
    "        long_probs =\\\n",
    "            torch.clamp(exponentiated_sys_utilities / long_denominators,\n",
    "                        min=self.min_comp_value,\n",
    "                        max=1.0)\n",
    "        return long_probs\n",
    "    \n",
    "    def calc_sys_utilities(self, inputs):\n",
    "        return self.linear(inputs)\n",
    "\n",
    "    def convert_long_to_wide(self, input_1d):\n",
    "        orig_num_rows = input_1d.size()[0]\n",
    "        new_num_rows =\\\n",
    "            int(orig_num_rows / self.num_alternatives)\n",
    "        wide_tensor = input_1d.view(new_num_rows, self.num_alternatives)\n",
    "        return wide_tensor\n",
    "\n",
    "    def convert_wide_to_long(self, input_2d):\n",
    "        orig_num_rows = input_2d.size()[0]\n",
    "        new_num_rows =\\\n",
    "            int(orig_num_rows) * self.num_alternatives\n",
    "        long_tensor = input_2d.view(new_num_rows, 1)\n",
    "        return long_tensor\n",
    "\n",
    "    def safe_logistic(self, inputs):\n",
    "        safe_input =\\\n",
    "            torch.clamp(inputs,\n",
    "                        min=self.min_exponent_val,\n",
    "                        max=self.max_exponent_val)\n",
    "        return torch.pow(1 + torch.exp(-1 * safe_input), -1)\n",
    "    \n",
    "    def safe_logit(self, inputs):\n",
    "        safe_input =\\\n",
    "            torch.clamp(inputs, min=self.min_comp_value, max=self.max_sub_1)\n",
    "        return torch.log(safe_input / (1 - safe_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "# Create the mapping matrices for pytorch\n",
    "####\n",
    "# Get the needed mapping matrix from pylogit, in the format used by pytorch\n",
    "row_to_obs_mnl_coo =\\\n",
    "    mnl_model.get_mappings_for_fit()['rows_to_obs'].tocoo()\n",
    "\n",
    "# Extract the sparsity structure information from the scipy.sparse matrix\n",
    "# for passing to pytorch\n",
    "concatenated_row_to_obs_iv_info =\\\n",
    "    torch.from_numpy(\n",
    "        np.concatenate((row_to_obs_mnl_coo.row[None, :],\n",
    "                        row_to_obs_mnl_coo.col[None, :]),\n",
    "                       axis=0)\n",
    "    ).long()\n",
    "\n",
    "# Get the non-zero values from the mapping matrix\n",
    "row_to_obs_mnl_data_torch =\\\n",
    "    torch.from_numpy(row_to_obs_mnl_coo.data.astype(np.float32)).double()\n",
    "\n",
    "# Create the mapping matrix in pytorch\n",
    "rows_to_obs_torch =\\\n",
    "    torch.sparse.FloatTensor(\n",
    "      concatenated_row_to_obs_iv_info,\n",
    "      row_to_obs_mnl_data_torch\n",
    "    ).double()\n",
    "\n",
    "# Initialize the pytorch MNL model\n",
    "torch_mnl_2 = MNL_Torch_v2(num_beta_params,\n",
    "                           num_alts,\n",
    "                           rows_to_obs_torch).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n",
      "Model's state_dict:\n",
      "('linear.weight', '\\t', torch.Size([1, 36]))\n",
      "Log-Likelihood values\n",
      "(-8419.371342840095, -8419.371351263893)\n"
     ]
    }
   ],
   "source": [
    "%pdb on\n",
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in torch_mnl_2.state_dict():\n",
    "    print(param_tensor, \"\\t\", torch_mnl_2.state_dict()[param_tensor].size())\n",
    "\n",
    "# Check that the pytorch MNL implementation works\n",
    "# Set the model parameters from the estimated pylogit params\n",
    "set_model_params(mnl_model.params.values, torch_mnl_2)\n",
    "\n",
    "# Compute probabilities\n",
    "x_train_torch = x_train_torch.double()\n",
    "torch_long_probs_2 = torch_mnl_2.forward(x_train_torch)\n",
    "# Get the probabilities as a numpy array\n",
    "numpy_long_probs_from_torch_2 = torch_long_probs_2.detach().numpy().ravel()\n",
    "# Make sure the probabilities are all close-ish\n",
    "np.allclose(numpy_long_probs_from_torch_2, mnl_model.long_fitted_probs)\n",
    "\n",
    "# Chcek that the log-likelihoods are equal-ish\n",
    "torch_log_like_2 = np.dot(mnl_model.choices,\n",
    "                        np.log(numpy_long_probs_from_torch_2.ravel()))\n",
    "print(\"Log-Likelihood values\")\n",
    "print(torch_log_like_2, mnl_model.llf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Woohoo I can use sparse matrices to recreate the pylogit mnl! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add a hidden layer to the MNL model\n",
    "- Note this hidden layer contain three \"neurons,\" each being a piecewise linear spline.\n",
    "- There will be one neuron per alternative for owning more than zero cars.\n",
    "- This will implement a different piecewise linear spline per utility function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiddenLayerMNL(MNL_Torch_v2):\n",
    "    def __init__(self,\n",
    "                 num_index_params,\n",
    "                 num_alternatives,\n",
    "                 rows_to_obs, # torch.sparse.FloatTensor\n",
    "                 num_hidden_units_per_spline=10,\n",
    "                 min_exponent_val=-700,\n",
    "                 max_exponent_val=700,\n",
    "                 max_comp_value=1e300,\n",
    "                 max_sub_1=1-1e16,\n",
    "                 min_comp_value=1e-300):\n",
    "        super(HiddenLayerMNL, self).__init__(\n",
    "            num_index_params,\n",
    "            num_alternatives,\n",
    "            rows_to_obs,\n",
    "            min_exponent_val=min_exponent_val,\n",
    "            max_exponent_val=max_exponent_val,\n",
    "            max_comp_value=max_comp_value,\n",
    "            max_sub_1=max_sub_1,\n",
    "            min_comp_value=min_comp_value\n",
    "        )\n",
    "        # Store needed constants for the model\n",
    "        self.num_hidden_units_per_spline = int(num_hidden_units_per_spline)\n",
    "        \n",
    "        # Organize info on the splines\n",
    "        self.spline_column_list = [-6, -5, -4]\n",
    "        self.num_splines = len(self.spline_column_list)\n",
    "\n",
    "        # Create parameters for the splines\n",
    "        self.spline_linear_coef_params =\\\n",
    "            torch.nn.Parameter(\n",
    "                torch.zeros(\n",
    "                    torch.Size([self.num_splines,\n",
    "                                num_hidden_units_per_spline]\n",
    "                                ),\n",
    "                    dtype=torch.double,\n",
    "                    requires_grad=True))\n",
    "\n",
    "        self.spline_knot_params =\\\n",
    "            torch.nn.Parameter(\n",
    "                torch.zeros(\n",
    "                    torch.Size([self.num_splines,\n",
    "                                num_hidden_units_per_spline]),\n",
    "                    dtype=torch.double,\n",
    "                    requires_grad=True))\n",
    "\n",
    "        # Combine all spline info\n",
    "        self.spline_info =\\\n",
    "            [(self.spline_linear_coef_params[x, :],\n",
    "              self.spline_knot_params[x, :],\n",
    "              self.spline_column_list[x]) for x in range(self.num_splines)]\n",
    "\n",
    "        # Create a ReLU layer\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Store computation layers as attributes\n",
    "        self.linear =\\\n",
    "            nn.Linear(self.num_index_params, 1, bias=False)\n",
    "        return None\n",
    "    \n",
    "    def calc_sys_utilities(self, inputs):\n",
    "        # Calculate X * beta\n",
    "        linear_term = self.linear(inputs)\n",
    "        \n",
    "        # Initialize the systematic utility values to be returned\n",
    "        sys_utility_vals = linear_term\n",
    "            \n",
    "        # Calculate the addition to the systematic utilities per spline\n",
    "        for pos, info in enumerate(self.spline_info):\n",
    "            # Unpack the info needed from the splines\n",
    "            spline_linear_coefs, spline_knot_params, col = info\n",
    "            \n",
    "            # Compute -1 * exp(spline_linear_coef_param)\n",
    "            # This ensures the derivative with respect to the transit\n",
    "            # access variable will be negative for the given alternatives\n",
    "            # that have 1, 2, or 3+ cars relative to zero cars\n",
    "            current_transformed_spline_linear_coefs =\\\n",
    "                -1 * torch.exp(torch.clamp(spline_linear_coefs,\n",
    "                                           min=self.min_exponent_val,\n",
    "                                           max=self.max_exponent_val))\n",
    "\n",
    "            # Compute max(0, [x - knot])\n",
    "            current_pre_spline_vals =\\\n",
    "                self.relu(inputs[:, col][:, None] - spline_knot_params[None, :])\n",
    "\n",
    "            # Compute \n",
    "            # sum_{hidden_units} {-1 * exp(spline_linear_coef_param) *\n",
    "            #                     max(0, [x - knot])}\n",
    "            current_spline_vals =\\\n",
    "                torch.mm(current_pre_spline_vals,\n",
    "                         current_transformed_spline_linear_coefs[:, None])\n",
    "                \n",
    "            # Increment the systematic utility values\n",
    "            sys_utility_vals = sys_utility_vals + current_spline_vals\n",
    "        return sys_utility_vals\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # Note we use clamp to guard against later over- or under-flow.\n",
    "        sys_utilities =\\\n",
    "            torch.clamp(self.calc_sys_utilities(inputs),\n",
    "                        min=self.min_exponent_val,\n",
    "                        max=self.max_exponent_val)\n",
    "\n",
    "        exponentiated_sys_utilities = torch.exp(sys_utilities)\n",
    "\n",
    "        # Get the denominators for calculating the probabilities\n",
    "        # There will be one entry per observation\n",
    "        denominators_by_obs =\\\n",
    "            torch.sparse.mm(self.rows_to_obs.transpose(0, 1),\n",
    "                            exponentiated_sys_utilities)\n",
    "        # Convert the denominators into an array of length sys_utilities.size[0]\n",
    "        long_denominators =\\\n",
    "            torch.sparse.mm(self.rows_to_obs, denominators_by_obs)\n",
    "\n",
    "        # Note we use clamp to guard against against zero probabilities.\n",
    "        long_probs =\\\n",
    "            torch.clamp(exponentiated_sys_utilities / long_denominators,\n",
    "                        min=self.min_comp_value,\n",
    "                        max=1.0)\n",
    "        return long_probs\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check that the new model is initialized at the MNL log likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "('spline_linear_coef_params', '\\t', torch.Size([3, 10]))\n",
      "('spline_linear_coef_params', '\\t', tensor([[-5., -5., -5., -5., -5., -5., -5., -5., -5., -5.],\n",
      "        [-5., -5., -5., -5., -5., -5., -5., -5., -5., -5.],\n",
      "        [-5., -5., -5., -5., -5., -5., -5., -5., -5., -5.]],\n",
      "       dtype=torch.float64))\n",
      "\n",
      "\n",
      "('spline_knot_params', '\\t', torch.Size([3, 10]))\n",
      "('spline_knot_params', '\\t', tensor([[5.0360, 5.3479, 5.5561, 6.0149, 6.0958, 6.2924, 6.5286, 6.6412, 6.8725,\n",
      "         7.1814],\n",
      "        [5.0360, 5.3479, 5.5561, 6.0149, 6.0958, 6.2924, 6.5286, 6.6412, 6.8725,\n",
      "         7.1814],\n",
      "        [5.0360, 5.3479, 5.5561, 6.0149, 6.0958, 6.2924, 6.5286, 6.6412, 6.8725,\n",
      "         7.1814]], dtype=torch.float64))\n",
      "\n",
      "\n",
      "('linear.weight', '\\t', torch.Size([1, 36]))\n",
      "('linear.weight', '\\t', tensor([[ 2.6264,  1.0124, -1.9606, -3.4084, -3.0713, -1.8843, -3.8212, -3.6161,\n",
      "         -0.5120,  0.5661,  0.2848,  0.6836, -1.5095, -2.9781, -3.2721, -1.8959,\n",
      "         -3.5720, -3.5108, -0.9951, -1.7182, -2.2863,  0.7917,  1.5112,  1.8265,\n",
      "          1.3261,  3.1611,  3.8959, -0.2626,  0.2713,  0.7713, -0.3569, -0.8650,\n",
      "         -0.8694,  0.3593,  0.7613,  1.1392]], dtype=torch.float64))\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Determine how many units per spline\n",
    "NUM_KNOTS = 10\n",
    "# Initialize the new model\n",
    "new_model = HiddenLayerMNL(num_beta_params,\n",
    "                           num_alts,\n",
    "                           rows_to_obs_torch,\n",
    "                           num_hidden_units_per_spline=NUM_KNOTS)\n",
    "\n",
    "####\n",
    "# Set the initial parameters\n",
    "####\n",
    "# Note the shapes of the parameters\n",
    "param_shape_dict_new_model =\\\n",
    "    {'spline_linear_coef_params': (3, NUM_KNOTS),\n",
    "     'spline_knot_params': (3, NUM_KNOTS),\n",
    "     'linear.weight': (1, 36)}\n",
    "\n",
    "# Create initial knot values\n",
    "initial_knot_vals =\\\n",
    "    (pd.Series(train_df_all['tran_access'].values\n",
    "                   [np.nonzero(train_df_all['tran_access'].values)])\n",
    "       .describe(percentiles=[0.05 + 0.1 * x\n",
    "                              for x in range(NUM_KNOTS)])).values[-11:-1]\n",
    "\n",
    "# Note the initial parameter values as a correctly ordered state dict\n",
    "initial_vals_new_model = OrderedDict()\n",
    "initial_vals_new_model['spline_linear_coef_params'] =\\\n",
    "     (torch.from_numpy(\n",
    "          np.ones((3, NUM_KNOTS)) * -5\n",
    "#           np.log(np.abs(mnl_model.params.values[[-6, -5, -4]][:, None]))\n",
    "                      )\n",
    "           .double())\n",
    "\n",
    "initial_vals_new_model['spline_knot_params'] =\\\n",
    "     (torch.from_numpy(\n",
    "         np.ones((3, NUM_KNOTS)) * initial_knot_vals[None, :])\n",
    "           .double()\n",
    "     )\n",
    "\n",
    "initial_vals_new_model['linear.weight'] =\\\n",
    "    torch.from_numpy(mnl_model.params.values[None, :]).double()\n",
    "\n",
    "# Note the initial parameter values as a correctly ordered numpy\n",
    "# array we can use to set the parameters with\n",
    "initial_vals_new_model_numpy =\\\n",
    "    np.concatenate(\n",
    "        tuple(t.data.numpy().ravel() for t in initial_vals_new_model.values()),\n",
    "        axis=0)\n",
    "\n",
    "# Set the model parameters\n",
    "set_model_params(initial_vals_new_model_numpy,\n",
    "                 new_model,\n",
    "                 param_shapes=param_shape_dict_new_model)\n",
    "new_model.double()\n",
    "\n",
    "# Look at the current params\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in new_model.state_dict():\n",
    "    print(param_tensor, \"\\t\", new_model.state_dict()[param_tensor].size())\n",
    "    print(param_tensor, \"\\t\", new_model.state_dict()[param_tensor])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Probs all close?', False)\n",
      "Log-Likelihood values\n",
      "(-8419.280009275899, -8419.371351263893)\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "# Check initial probabilities with new model\n",
    "####\n",
    "\n",
    "# Compute probabilities\n",
    "x_train_torch = x_train_torch.double()\n",
    "torch_long_probs_new = new_model.forward(x_train_torch)\n",
    "# Get the probabilities as a numpy array\n",
    "numpy_long_probs_from_torch_new = torch_long_probs_new.data.numpy().ravel()\n",
    "# Make sure the probabilities are all close-ish\n",
    "print(\"Probs all close?\",\n",
    "      np.allclose(numpy_long_probs_from_torch_new, mnl_model.long_fitted_probs))\n",
    "\n",
    "# Chcek that the log-likelihoods are equal-ish\n",
    "torch_log_like_new = np.dot(mnl_model.choices,\n",
    "                        np.log(numpy_long_probs_from_torch_new.ravel()))\n",
    "print(\"Log-Likelihood values\")\n",
    "print(torch_log_like_new, mnl_model.llf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate the new model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a loss for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegLogLikelihood(nn.Module):\n",
    "    def __init__(self):\n",
    "        nn.Module.__init__(self)\n",
    "        \n",
    "    def forward(self, probs, targets):\n",
    "        total_loss =\\\n",
    "            (-1 * torch.dot(torch.flatten(targets),\n",
    "                            torch.log(torch.flatten(probs))))\n",
    "        return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss value: 8,419.28001\n"
     ]
    }
   ],
   "source": [
    "# Set up a loss for pytorch training\n",
    "pytorch_loss = NegLogLikelihood()\n",
    "\n",
    "# Determine how many units per spline\n",
    "NUM_KNOTS = 10\n",
    "\n",
    "# Initialize the new model\n",
    "new_model = HiddenLayerMNL(num_beta_params,\n",
    "                           num_alts,\n",
    "                           rows_to_obs_torch,\n",
    "                           num_hidden_units_per_spline=NUM_KNOTS)\n",
    "\n",
    "####\n",
    "# Set the initial parameters\n",
    "####\n",
    "# Note the shapes of the parameters\n",
    "param_shape_dict_new_model =\\\n",
    "    {'spline_linear_coef_params': (3, NUM_KNOTS),\n",
    "     'spline_knot_params': (3, NUM_KNOTS),\n",
    "     'linear.weight': (1, 36)}\n",
    "\n",
    "# Create initial knot values that are the percentiels of the transit access variable\n",
    "initial_knot_vals =\\\n",
    "    (pd.Series(train_df_all['tran_access'].values\n",
    "                   [np.nonzero(train_df_all['tran_access'].values)])\n",
    "       .describe(percentiles=[0.05 + 0.1 * x\n",
    "                              for x in range(NUM_KNOTS)])).values[-11:-1]\n",
    "\n",
    "# Note the initial parameter values as a correctly ordered state dict\n",
    "initial_vals_new_model = OrderedDict()\n",
    "\n",
    "# Start the spline coefficients near zero to be close to the MNL\n",
    "initial_vals_new_model['spline_linear_coef_params'] =\\\n",
    "     (torch.from_numpy(\n",
    "         np.ones((3, NUM_KNOTS)) * -5\n",
    "                      )\n",
    "           .double())\n",
    "\n",
    "initial_vals_new_model['spline_knot_params'] =\\\n",
    "     (torch.from_numpy(\n",
    "         np.ones((3, NUM_KNOTS)) * initial_knot_vals[None, :])\n",
    "           .double()\n",
    "     )\n",
    "\n",
    "initial_vals_new_model['linear.weight'] =\\\n",
    "    torch.from_numpy(mnl_model.params.values[None, :]).double()\n",
    "\n",
    "# Note the initial parameter values as a correctly ordered numpy\n",
    "# array we can use to set the parameters with\n",
    "initial_vals_new_model_numpy =\\\n",
    "    np.concatenate(\n",
    "        tuple(t.data.numpy().ravel() for t in initial_vals_new_model.values()),\n",
    "        axis=0)\n",
    "\n",
    "# Set the initial model parameters\n",
    "set_model_params(initial_vals_new_model_numpy,\n",
    "                 new_model,\n",
    "                 param_shapes=param_shape_dict_new_model)\n",
    "new_model.double()\n",
    "\n",
    "# Make sure the input and model has the correct type\n",
    "x_train_torch = x_train_torch.double()\n",
    "y_train_torch = y_train_torch.double()\n",
    "\n",
    "####\n",
    "# Create numpy compatible loss and gradient\n",
    "####\n",
    "# Create the closure for the model\n",
    "scipy_loss_and_grad =\\\n",
    "    make_scipy_closure(\n",
    "        x_train_torch,\n",
    "        y_train_torch,\n",
    "        new_model,\n",
    "        pytorch_loss,\n",
    "        param_shapes=param_shape_dict_new_model)\n",
    "\n",
    "# Look at the initial loss value\n",
    "initial_loss_val = pytorch_loss(new_model(x_train_torch), y_train_torch)\n",
    "print(\"Initial loss value: {:,.5f}\".format(initial_loss_val.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform the minimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH\n",
      "Final loss value: 8,406.65938\n",
      "Gradient: \n",
      "[ 2.58630451e-03 -3.96025773e-03 -7.98353566e-02  3.62998187e-03\n",
      "  5.89447511e-02  1.28600045e-01  4.00684022e-02  3.06070033e-02\n",
      "  3.41771873e-03  0.00000000e+00 -6.02721186e-02  1.12701641e-03\n",
      " -2.20122802e-03 -1.36466783e-01 -1.48651344e-01 -1.96691398e-01\n",
      " -1.38994076e-01 -1.45014883e-01 -5.54919714e-02 -1.10505520e-04\n",
      "  3.12116529e-02  8.58672871e-02  2.01121141e-01  2.00103697e-01\n",
      "  1.59783605e-01  9.35705118e-02  5.23335787e-02  3.18080366e-02\n",
      "  1.12294035e-02  1.10633133e-04 -7.03945489e-02 -3.44335838e-01\n",
      "  8.85769888e-02 -5.44912445e-01  3.32674590e-02 -5.63685186e-01\n",
      " -2.66543890e-03  2.03333017e-01 -2.87217193e-02  0.00000000e+00\n",
      "  6.07504044e-02 -5.71956667e-03 -8.44409509e-02  3.93913150e-01\n",
      " -3.36398490e-01 -4.51719602e+00 -3.14267533e-01  4.18182111e-01\n",
      "  6.06023938e-02  9.49292087e-04 -2.90185355e-02 -8.31567857e-02\n",
      "  1.62975558e-01 -4.86611441e-02 -3.65618709e-01 -6.13545256e-03\n",
      "  1.09470589e-01 -3.08885572e-02 -4.96484709e-03 -4.07317673e-04\n",
      " -9.41083814e-01  5.34167864e-01  5.32098938e-01 -5.13685525e-01\n",
      " -7.18690601e-02 -1.70449207e-01  1.96769181e-01 -5.71436491e-01\n",
      " -8.23129427e-02 -2.96184716e-01 -1.74414191e-01 -5.59962316e-01\n",
      "  5.45001331e-01  3.18250995e-01 -1.89180108e-01 -7.95216085e-01\n",
      " -3.22891239e-01  6.11877928e-02  1.15397773e-01  4.05108489e-01\n",
      " -6.70839033e-01 -1.35848282e-01  1.11544566e-01 -1.18217903e-01\n",
      " -4.62286473e-02  4.48187318e-01 -1.21708134e-01  3.27999338e-01\n",
      " -2.22205977e-01 -2.53489221e-01 -3.34554540e+00  2.60890819e+00\n",
      " -1.61190420e+00 -3.29827658e-01  2.84080872e-02 -6.86763571e-01]\n",
      "Gradient norm: 6.945287\n"
     ]
    }
   ],
   "source": [
    "# Perform the minimization\n",
    "results =\\\n",
    "    minimize(\n",
    "        scipy_loss_and_grad, initial_vals_new_model_numpy, method='l-bfgs-b', jac=True)\n",
    "print(results['message'])\n",
    "\n",
    "# Look at the current loss value\n",
    "current_loss_val = pytorch_loss(new_model(x_train_torch), y_train_torch)\n",
    "print(\"Final loss value: {:,.5f}\".format(current_loss_val.data))\n",
    "\n",
    "# Look at the current gradient and its norm to assess convergence.\n",
    "grad = get_numpy_grad(new_model)\n",
    "grad_norm = np.linalg.norm(grad)\n",
    "print(\"Gradient: \\n{}\".format(grad))\n",
    "print(\"Gradient norm: {:,.6f}\".format(grad_norm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at the estimated parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "('spline_linear_coef_params', '\\t', torch.Size([3, 10]))\n",
      "('spline_linear_coef_params', '\\t', tensor([[-3.5409, -2.1945, -2.5927, -1.9343, -2.1892, -1.2520, -2.4435, -3.0483,\n",
      "         -4.9526, -4.9936],\n",
      "        [-4.0424, -4.8904, -4.5543, -2.4991, -2.4132, -0.4969, -2.4812, -2.4393,\n",
      "         -3.3195, -4.9148],\n",
      "        [-3.2000, -2.1590, -1.2807, -1.1506, -1.3950, -2.0534, -2.6010, -3.1493,\n",
      "         -4.3098, -4.8112]], dtype=torch.float64))\n",
      "\n",
      "\n",
      "('spline_knot_params', '\\t', torch.Size([3, 10]))\n",
      "('spline_knot_params', '\\t', tensor([[6.2153, 6.2477, 5.1378, 6.2305, 5.9017, 5.5140, 5.5036, 5.5711, 5.4914,\n",
      "         7.7384],\n",
      "        [5.2662, 6.4101, 6.4946, 6.0159, 6.0136, 6.5092, 6.0140, 6.0155, 6.0600,\n",
      "         7.3999],\n",
      "        [5.8985, 5.9290, 6.0277, 6.1424, 6.1287, 6.0660, 6.0065, 5.9319, 5.7958,\n",
      "         6.4116]], dtype=torch.float64))\n",
      "\n",
      "\n",
      "('linear.weight', '\\t', torch.Size([1, 36]))\n",
      "('linear.weight', '\\t', tensor([[-0.4566, -0.7147, -4.2658, -3.4245, -3.0945, -2.1271, -3.6082, -3.4149,\n",
      "         -0.2869,  0.4865,  0.1818,  0.5546, -1.4579, -2.9243, -3.2164, -1.8894,\n",
      "         -3.5892, -3.4932, -0.9898, -1.7049, -2.4139,  0.7974,  1.5176,  1.7681,\n",
      "          1.3631,  3.2034,  3.8869, -0.2629,  0.2634,  0.6944,  0.2288, -0.5474,\n",
      "         -0.3781,  0.3530,  0.7527,  1.1174]], dtype=torch.float64))\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Look at the current params\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in new_model.state_dict():\n",
    "    print(param_tensor, \"\\t\", new_model.state_dict()[param_tensor].size())\n",
    "    print(param_tensor, \"\\t\", new_model.state_dict()[param_tensor])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.13783575 5.49135603 5.50362383 5.51403638 5.57105688 5.90168814\n",
      " 6.21531294 6.23048093 6.24767389 7.73843451]\n",
      "[5.26619363 6.01358719 6.01397601 6.01552791 6.01586363 6.05996155\n",
      " 6.41010307 6.4946217  6.50915594 7.39991148]\n",
      "[5.79580191 5.89852388 5.92903363 5.93186106 6.00649999 6.02765559\n",
      " 6.0659523  6.12872403 6.14244136 6.41156787]\n"
     ]
    }
   ],
   "source": [
    "# Look at the final knot positions for each alternative\n",
    "final_spline_knot_params = new_model.spline_knot_params.data.numpy()\n",
    "\n",
    "for i in [0, 1, 2]:\n",
    "    print(np.sort(final_spline_knot_params[i, :]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, from above I see that I should probably implement a transformation such that the knot positions stay inside the range of the observed data. Otherwise the knot position could go outside the observed data and then start getting gradients of zero..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at the gradient of the estimated parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grad_flow(named_parameters):\n",
    "    ave_grads = []\n",
    "    layers = []\n",
    "    for n, p in named_parameters:\n",
    "        if(p.requires_grad) and (\"bias\" not in n):\n",
    "            layers.append(n)\n",
    "            ave_grads.append(p.grad.abs().mean())\n",
    "    plt.plot(ave_grads, alpha=0.3, color=\"b\")\n",
    "    plt.hlines(0, 0, len(ave_grads)+1, linewidth=1, color=\"k\" )\n",
    "    plt.xticks(range(0,len(ave_grads), 1), layers, rotation=\"vertical\")\n",
    "    plt.xlim(xmin=0, xmax=len(ave_grads))\n",
    "    plt.xlabel(\"Layers\")\n",
    "    plt.ylabel(\"average gradient\")\n",
    "    plt.title(\"Gradient flow\")\n",
    "    plt.grid(True)\n",
    "    return None\n",
    "    \n",
    "\n",
    "def plot_grad_flow_v2(named_parameters):\n",
    "    '''Plots the gradients flowing through different layers in the net during training.\n",
    "    Can be used for checking for possible gradient vanishing / exploding problems.\n",
    "    \n",
    "    Usage: Plug this function in Trainer class after loss.backwards() as \n",
    "    \"plot_grad_flow(self.model.named_parameters())\" to visualize the gradient flow'''\n",
    "    ave_grads = []\n",
    "    max_grads= []\n",
    "    layers = []\n",
    "    for n, p in named_parameters:\n",
    "        if(p.requires_grad) and (\"bias\" not in n):\n",
    "            layers.append(n)\n",
    "            ave_grads.append(p.grad.abs().mean())\n",
    "            max_grads.append(p.grad.abs().max())\n",
    "    plt.bar(np.arange(len(max_grads)), max_grads, alpha=0.1, lw=1, color=\"c\")\n",
    "    plt.bar(np.arange(len(max_grads)), ave_grads, alpha=0.1, lw=1, color=\"b\")\n",
    "    plt.hlines(0, 0, len(ave_grads)+1, lw=2, color=\"k\" )\n",
    "    plt.xticks(range(0,len(ave_grads), 1), layers, rotation=\"vertical\")\n",
    "    plt.xlim(left=0, right=len(ave_grads))\n",
    "    plt.ylim(bottom = -0.001, top=0.02) # zoom in on the lower gradient regions\n",
    "    plt.xlabel(\"Layers\")\n",
    "    plt.ylabel(\"average gradient\")\n",
    "    plt.title(\"Gradient flow\")\n",
    "    plt.grid(True)\n",
    "    plt.legend([Line2D([0], [0], color=\"c\", lw=4),\n",
    "                Line2D([0], [0], color=\"b\", lw=4),\n",
    "                Line2D([0], [0], color=\"k\", lw=4)], ['max-gradient', 'mean-gradient', 'zero-gradient'])\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss value: 8,406.65938\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "####\n",
    "# Examine the gradients of the new model\n",
    "####\n",
    "# Ensure we start calculating gradients at zero\n",
    "new_model.zero_grad()\n",
    "# Look at the initial loss value\n",
    "current_loss_val = pytorch_loss(new_model(x_train_torch), y_train_torch)\n",
    "# Get gradients of the loss\n",
    "current_loss_val.backward()\n",
    "print(\"Loss value: {:,.5f}\".format(current_loss_val.data))\n",
    "plot_grad_flow(new_model.named_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recap\n",
    "\n",
    "Okay. I'm able to estimate something. The final gradient, while not zero, isn't a horrendously huge number. The training loss has decreased, which is encouraging. And finally, the parameters have made significant moves from their starting values into reasonable terminating values.\n",
    "\n",
    "However, the performance is still nothing like the random forest. I'm assuming this means the discrete features are carrying all the predictive power and I should make use of them?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Random Forests\n",
    "\n",
    "The basic idea is to use gradient boosting to add the random forests to the systematic utilities of the choice model.\n",
    "\n",
    "We'll use one step of gradient boosting where the \"weak learner\" is a random forest. Note we use only one step to stay far from overfitting.\n",
    "\n",
    "Also, there is one random forest per systematic utility, and these forests only use the discrete variables that are present in the linear portion of that systematic utility. This is done to avoid mother logit complaints and to avoid any issues with illogical elasticities with respect to transit access. The random forest simply will not be based on transit access at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_design_columns_for_rf(\n",
    "    model_obj, model_spec, alt_id, choice_label_dict):\n",
    "    \"\"\"\n",
    "    Gets all the column indices from model_obj.design that\n",
    "    correspong to non-intercept and non-tran_access variables\n",
    "    for a given alternative. These are the discrete variable\n",
    "    columns for each alternative's systematic utilities.\n",
    "    \"\"\"\n",
    "    # Get the model parameter names\n",
    "    model_param_name_list = model_obj.params.index.values.tolist()\n",
    "    # Determine the columns to pay attention to\n",
    "    col_list = []\n",
    "    for col_name in model_spec:\n",
    "        # Skip the intercept column as its the same value for everyone\n",
    "        # Skip trans_access as we just want discrete values\n",
    "        if col_name in ['intercept', 'tran_access']:\n",
    "            continue\n",
    "        # Get the name of the associated model parameter for this column\n",
    "        param_name = col_name + ' ({})'.format(choice_label_dict[alt_id])\n",
    "        # Get the design matrix column corresponding to this parameter\n",
    "        if param_name not in model_param_name_list:\n",
    "            continue\n",
    "        design_column_idx = model_param_name_list.index(param_name)\n",
    "        # Store the identified column\n",
    "        col_list.append(design_column_idx)\n",
    "    return col_list\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: [3, 6],\n",
       " 2: [4, 7, 9, 12, 15, 18, 21, 24, 27, 33],\n",
       " 3: [8, 10, 13, 16, 19, 22, 25, 28, 34],\n",
       " 4: [5, 11, 14, 17, 20, 23, 26, 29, 35]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test that the function works\n",
    "rf_design_cols =\\\n",
    "    {alt: get_design_columns_for_rf(mnl_model, spec_dict, alt, choice_labels)\n",
    "     for alt in range(1, 5)}\n",
    "\n",
    "\n",
    "rf_design_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current residuals (aka the gradient of the log-likelihoods\n",
    "# with respect to the systematic utilities)\n",
    "mnl_plus_net_residuals =\\\n",
    "    (y_train_torch - new_model(x_train_torch)).data.numpy().ravel()\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "np.random.seed(452)\n",
    "\n",
    "# Subsample the data based on the observation ids\n",
    "forest_subsample_decimal = 0.75\n",
    "train_obs_ids = np.sort(train_df_all[mnl_model.obs_id_col].unique())\n",
    "num_subsamples = int(forest_subsample_decimal * train_obs_ids.size)\n",
    "\n",
    "subsampled_ids =\\\n",
    "    np.random.choice(train_obs_ids, size=num_subsamples, replace=False)\n",
    "subsampled_row_indicator =\\\n",
    "    train_df_all[mnl_model.obs_id_col].isin(subsampled_ids).values\n",
    "subsampled_train_df =\\\n",
    "    train_df_all.loc[subsampled_row_indicator]\n",
    "\n",
    "# Determine the columns to be used in the random forests, aka all the\n",
    "# discrete variables from an alternative's systematic utilities\n",
    "rf_design_cols =\\\n",
    "    {alt: get_design_columns_for_rf(mnl_model, spec_dict, alt, choice_labels)\n",
    "     for alt in range(1, 5)}\n",
    "\n",
    "\n",
    "#####\n",
    "# Use the subsampled data to predict the residuals for each observation\n",
    "#####\n",
    "# Initialize a  dictionary to store the random forest objects after creation\n",
    "alt_to_random_forest_dict = {}\n",
    "\n",
    "# Initialize a column for the predicted residuals from the random forests\n",
    "rf_prediction_col = 'pred_rf_residuals'\n",
    "train_df_all[rf_prediction_col] = 0\n",
    "\n",
    "for alt in range(1, 5):\n",
    "    # Figure out what rows of the training data . correspond to this alternative\n",
    "    alt_indicator_full_data = (train_df_all[mnl_model.alt_id_col] == alt).values\n",
    "    # Get the subsampled training data for this alternative's random forest.\n",
    "    subsampled_x_for_rf =\\\n",
    "        mnl_model.design[subsampled_row_indicator *\n",
    "                         alt_indicator_full_data, :][:, rf_design_cols[alt]]\n",
    "    subsampled_y_for_rf =\\\n",
    "        mnl_plus_net_residuals[subsampled_row_indicator * alt_indicator_full_data]\n",
    "    # Build a random forest to predict the residuals\n",
    "    random_forest = RandomForestRegressor(n_estimators=100, min_samples_leaf=100)\n",
    "    random_forest.fit(subsampled_x_for_rf, subsampled_y_for_rf)\n",
    "    # Save the forest\n",
    "    alt_to_random_forest_dict[alt] = random_forest\n",
    "    # Make and store predictions on the complete dataset\n",
    "    full_x_for_rf =\\\n",
    "        mnl_model.design[alt_indicator_full_data, :][:, rf_design_cols[alt]]\n",
    "    train_df_all.loc[alt_indicator_full_data, rf_prediction_col] =\\\n",
    "        random_forest.predict(full_x_for_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "# Add the MNL + Net systematic utilities to the dataset\n",
    "####\n",
    "# Note the clamping guards against overflow and underflow\n",
    "train_df_all['mnl_plus_net_systematic_utilities'] =\\\n",
    "    torch.clamp(new_model.calc_sys_utilities(x_train_torch),\n",
    "                min=new_model.min_exponent_val,\n",
    "                max=new_model.max_exponent_val).data.numpy().ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a final model based on the new random forest predictions and previous\n",
    "# MNL + Neural Net predictions\n",
    "final_spec_dict, final_name_dict = OrderedDict(), OrderedDict()\n",
    "\n",
    "# Add needed predictors to the specification and name dictionaries\n",
    "final_spec_dict['mnl_plus_net_systematic_utilities'] = [[1, 2, 3, 4]]\n",
    "final_name_dict['mnl_plus_net_systematic_utilities'] = ['mnl_plus_net_sys_utilities']\n",
    "\n",
    "# We exclude alternative 1 for identifiability since the residuals must sum to zero\n",
    "final_spec_dict[rf_prediction_col] = [2, 3, 4]\n",
    "final_name_dict[rf_prediction_col] =\\\n",
    "     [rf_prediction_col + ' ({})'.format(choice_labels[x]) for x in final_spec_dict[rf_prediction_col]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-likelihood at zero: -14,528.3649\n",
      "Initial Log-likelihood: -8,406.6594\n",
      "Estimation Time for Point Estimation: 0.03 seconds.\n",
      "Final log-likelihood: -8,365.6404\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Multinomial Logit Model Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>      <td>choiceBoolean</td>      <th>  No. Observations:  </th>   <td>10,480</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>         <td>Multinomial Logit Model</td> <th>  Df Residuals:      </th>   <td>10,476</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                  <td>MLE</td>           <th>  Df Model:          </th>      <td>4</td>     \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 13 Jan 2020</td>     <th>  Pseudo R-squ.:     </th>    <td>0.424</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>06:48:42</td>         <th>  Pseudo R-bar-squ.: </th>    <td>0.424</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>AIC:</th>                 <td>16,739.281</td>        <th>  Log-Likelihood:    </th> <td>-8,365.640</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>BIC:</th>                 <td>16,768.310</td>        <th>  LL-Null:           </th> <td>-14,528.365</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "               <td></td>                  <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mnl_plus_net_sys_utilities</th>  <td>    1.0000</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>pred_rf_residuals (1 Car)</th>   <td>    5.5040</td> <td>    0.798</td> <td>    6.898</td> <td> 0.000</td> <td>    3.940</td> <td>    7.068</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>pred_rf_residuals (2 Cars)</th>  <td>    5.0455</td> <td>    1.187</td> <td>    4.250</td> <td> 0.000</td> <td>    2.719</td> <td>    7.372</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>pred_rf_residuals (3+ Cars)</th> <td>    1.1404</td> <td>    8.390</td> <td>    0.136</td> <td> 0.892</td> <td>  -15.303</td> <td>   17.584</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                     Multinomial Logit Model Regression Results                    \n",
       "===================================================================================\n",
       "Dep. Variable:               choiceBoolean   No. Observations:               10,480\n",
       "Model:             Multinomial Logit Model   Df Residuals:                   10,476\n",
       "Method:                                MLE   Df Model:                            4\n",
       "Date:                     Mon, 13 Jan 2020   Pseudo R-squ.:                   0.424\n",
       "Time:                             06:48:42   Pseudo R-bar-squ.:               0.424\n",
       "AIC:                            16,739.281   Log-Likelihood:             -8,365.640\n",
       "BIC:                            16,768.310   LL-Null:                   -14,528.365\n",
       "===============================================================================================\n",
       "                                  coef    std err          z      P>|z|      [0.025      0.975]\n",
       "-----------------------------------------------------------------------------------------------\n",
       "mnl_plus_net_sys_utilities      1.0000        nan        nan        nan         nan         nan\n",
       "pred_rf_residuals (1 Car)       5.5040      0.798      6.898      0.000       3.940       7.068\n",
       "pred_rf_residuals (2 Cars)      5.0455      1.187      4.250      0.000       2.719       7.372\n",
       "pred_rf_residuals (3+ Cars)     1.1404      8.390      0.136      0.892     -15.303      17.584\n",
       "===============================================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the final model object\n",
    "final_mnl_model =\\\n",
    "    pl.create_choice_model(train_df_all,\n",
    "                           ALT_ID_COL,\n",
    "                           OBS_ID_COL,\n",
    "                           CHOICE_COL,\n",
    "                           final_spec_dict,\n",
    "                           'MNL',\n",
    "                           names=final_name_dict,\n",
    "                          )\n",
    "\n",
    "# Fit the model\n",
    "num_params_final_mnl = final_mnl_model.design.shape[1]\n",
    "init_values = np.zeros(num_params_final_mnl)\n",
    "# Start off exactly matching the MNL + Net predictions\n",
    "init_values[0] = 1\n",
    "\n",
    "final_mnl_model.fit_mle(init_values, constrained_pos=[0])\n",
    "\n",
    "# Show model estimation results\n",
    "final_mnl_model.get_statsmodels_summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
